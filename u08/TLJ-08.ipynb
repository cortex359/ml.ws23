{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung 8\n",
    "\n",
    "**Gruppenname:**\n",
    "\n",
    "*TLJ*\n",
    "\n",
    "Christian Rene Thelen, Artur Less, Karl Johannes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Entscheidungsbaum - Information Gain\n",
    "\n",
    "Ein Freundeskreis von Studierenden wandert gerne. Sie haben sich in der Vergangenheit notiert, welche Wetterbedingungen bei ihren Wanderungen herrschten:\n",
    "\n",
    "![Decision Tree Table](https://data.bialonski.de/ml/table-decision-tree-exercise.png)\n",
    "\n",
    "Eine Studentin möchte einen Entscheidungsbaum aufbauen, um besser zu verstehen, unter welchen Bedingungen die Gruppe wandert. Wir werden uns hier auf den ersten Schnitt im entstehenden Entscheidungsbaum beschränken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ihre Aufgaben:**\n",
    "\n",
    "(1) Schlagen Sie in den Folien der Vorlesung nach, wie der Information Gain definiert ist, und geben Sie ihn hier **in eigenen Worten** wieder."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als **Information Gain** (oder auch Mutual Information) wird die Abnahme des mittleren Informationsgehalts eines Ergebnisses der Zufallsvariablen $X$ durch Kenntnis des Ergebnisses einer Zufallsvariablen $Y$ genannt. \n",
    "$$\n",
    "\t\\text{IG}(X, Y) = H(X) - H(X|Y)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Bestimmen Sie die Entropie für \"Wandern gehen\" (und geben Sie ihren Wert hier an)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die **Entropie** $H(X)$ ist der mittlere Informationsgehalt eines Ereignisses (Ausgangs) eines Zufallsexperiments mit Zufallsvariable $X$. \n",
    "$$\n",
    "\tH(X) = E(I) = -\\sum_{k=1}^C p_k \\log_2 (p_k)\n",
    "$$\n",
    "\n",
    "Hier bedeutet das, für $X=\\{\\text{Wandern gehen}\\}$ ist die Entropie\n",
    "$$\n",
    "H(X) = -\\left(\\frac{3}{5}\\log_2 \\left(\\frac{3}{5}\\right) + \\frac{2}{5}\\log_2 \\left(\\frac{2}{5}\\right)\\right) \\approx 0.97095\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Bestimmen Sie die bedingten Entropien für \"Wandern gehen\" bezüglich der Features und geben Sie die Werte, die Sie erhalten haben, an."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die **bedingte Entropie** $H(X|Y)$ ist der mittlere Informationsgehalt eines Ergebnisses einer Zufallsvariablen $X$ unter der Bedingung, dass der Wert einer Zufallsvariablen $Y$ bekannt ist. \n",
    "$$\\begin{aligned}\n",
    "\tH(X|Y) &= \\sum_j p(y_j) \\cdot H(X|Y=y_j) \\\\\n",
    "\t&= - \\sum_i \\sum_j p(y_j)\\cdot p(x_i|y_i)\\cdot\\log_2 \\left( p(x_i|y_j) \\right) \\\\\n",
    "\t&= - \\sum_{i, j} p(x_i, y_j) \\log_2 \\frac{p(x_i, y_j)}{p(y_j)}\n",
    "\\end{aligned}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gesucht ist nach $H(X|Y)$ für $$\n",
    "\\begin{aligned}\n",
    "Y_{W_S} &= \\{\\text{Wetter} = \\text{Sonne}\\} \\\\\n",
    "H(X|Y_{W_S}) &= \n",
    "Y_{W_R} &= \\{\\text{Wetter} = \\text{Regen}\\} \\\\\n",
    "Y_{T_h} &= \\{\\text{Temperatur} = \\text{hoch}\\} \\\\\n",
    "Y_{T_m} &= \\{\\text{Temperatur} = \\text{mild}\\} \\\\\n",
    "Y_{W} &= \\{\\text{Wind}\\}\\\\\n",
    "\\overline{Y_{W}} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Bestimmen Sie den Information Gain für jedes Feature und geben sie die Werte hier an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Basierend auf Ihrer Beobachtung in Schritt (4): Welches Feature wird für eine Teilung in zwei Äste benutzt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Entscheidungsbäume (CART-Algorithmus)\n",
    "\n",
    "In dieser Aufgabe werden Sie einen binären Entscheidungsbaum, wie Sie ihn in der Vorlesung kennengelernt haben, selbst implementieren.\n",
    "Sie werden den Algorithmus CART implementieren und die Shannon-Entropie, bzw. den **Information Gain (IG)** als Metrik zum Aufbau des Baumes verwenden.<br>\n",
    "Der CART Algorithmus ist nicht auf die Verwendung des IG als Metrik angewiesen und kann zum Beispiel auch mit dem **Gini Unreinheit** umgesetzt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ihre Daten**<br>\n",
    "Sie werden erneut den Schwertlilien Datensatz untersuchen, den Sie bereits in einer der ersten Übungen kennengelernt haben.\n",
    "Zur Erinnerung: Der Datensatz enthält $N=150$ Datenpunkte mit jeweils drei Features. Jede Schwertlilie kann genau einer der drei Schwertlilienarten angehören ($C=3$ Klassen).\n",
    "Ihre Aufgabe wird es sein, einen **Klassifikationsbaum** zu trainieren, der die drei Klassen basierend auf zwei ausgewählten Features ('Petal width' und 'Sepal width') klassifiziert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweise zur Nomenklatur**<br>\n",
    "Alle Begriffe und Definitionen, die in dieser Übung benötigt werden, haben Sie in der Vorlesung kennengelernt.\n",
    "Schauen Sie daher bei Unklarheiten noch einmal in die Folien.\n",
    "Die wichtigsten Formeln sind in den Teilaufgaben noch einmal aufgeschrieben.<br>\n",
    "Der Datensatz mit $N=150$ Datenpunkten setzt sich zusammen aus den Labels $y\\in\\{0,1,2\\}^N$ und einer Feature-Matrix mit 2 Features.\n",
    "Die Matrix hat damit eine Größe von $N\\times 2=150 \\times 2$.\n",
    "Wir bezeichnen mit $x_{i,j}$ den Wert des $j$-ten Features des $i$-ten Datenpunkts und mit $x_{:,j}$ den Feature-Vektor des $j$-ten Features für alle Datenpunkte.\n",
    "\n",
    "Es werden in den Aufgaben folgende Kurzschreibweisen verwendet:<br>\n",
    "$p(y=0)=\\frac{\\big|\\{y_i|y_i=0\\}\\big|}{\\big|y\\big|}$<br>\n",
    "$p(x_{:,j} \\lt z)=\\frac{\\big|\\{x_{i,j}|x_{i,j} \\lt z\\}\\big|}{\\big|x_{:,j}\\big|}$<br>\n",
    "$p(y=0|x_{:,j} \\lt z)=\\frac{\\big|\\{y_i|y_i=0,\\: x_{i,j} \\lt z\\}\\big|}{\\big|\\{y_i|x_{i,j} \\lt z\\}\\big|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CART-Algorithmus (Überblick)**<br>\n",
    "CART erstellt Klassifikationsbäume rekursiv nach folgendem Schema (Details finden Sie in den Teilaufgaben):\n",
    "1. Falls eines der Abbruchkriterien erfüllt ist, breche den aktuellen Rekursionsschritt ab.\n",
    "2. Berechne für jeden möglichen binären Split der Daten im aktuellen Knoten den Information Gain (IG) als Differenz zwischen der Entropie des aktuellen Datensatzes und den Entropien der beiden Teil-Datensätze, die aus einem Split des Datensatzes hervorgehen.<br>\n",
    "   IG bei Split des Datensatzes am Grenzwert $z$ des Features $x_{:,j}$: $$H(Y)-p(x_{:,j} \\lt z)H(Y|x_{:,j} \\lt z)- p(x_{:,j} \\ge z)H(Y|x_{:,j} \\ge z)$$\n",
    "3. Splitte den aktuellen Knoten an dem Grenzwert $z^*$ des Features $x_{:,j}$, mit dem der größte Information Gain erzielt wird. Daraus resultieren zwei neue, kleinere Knoten mit Teil-Datensätzen: $\\{y_i,(x_{i,1},x_{i,2})|x_{i,j} \\lt z\\}$ und $\\{y_i,(x_{i,1},x_{i,2})|x_{i,j} \\ge z\\}$\n",
    "4. Wiederhole die Rekursion mit den beiden neuen Knoten und Teil-Datensätzen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ihre Aufgaben**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Zur Beurteilung eines Splits benötigen wir zunächst Funktionen, die die Entropie und den Information Gain berechnen.\n",
    "\n",
    "Schreiben Sie eine Funktion `entropy`, welche das Ergebnis eines Zufallsexperiments mit Zufallsvariable `Y` als Parameter übergeben bekommt und die Entropie `H(Y)` berechnet.\n",
    "Hier noch einmal die Formel zur Berechnung der Entropie eines Ereignisses `Y`, das die Werte zwischen `1` und `C` annehmen kann:\n",
    "\n",
    "$$\n",
    "H(Y) = - \\sum_{k=1}^C {p(y=k)~\\log_2{p(y=k)}}\n",
    "$$\n",
    "\n",
    "**Hinweis:**<br>\n",
    "Bedenken Sie den Sonderfall, wenn alle Datenpunkte das gleiche Label haben.\n",
    "\n",
    "**Anschaulich:**<br>\n",
    "Sie haben die Entropie in der Vorlesung als *mittleren Informationsgehalt* des Ereignisses einer Zufallsvariable kennengelernt.\n",
    "Eine weitere Interpretation der Entropie stellt diese als *mittlere Unsicherheit* über das Ereignis dar und beantwortet die Frage: Wie unsicher ist das Ergebnis der zugrundeliegenden Zufallsvariable?\n",
    "Falls das Ergebnis unsicher ist (wie das Ergebnis eines einfachen Münzwurfs, bei dem wir nicht wissen ob Zahl oder Kopf oben liegen wird), ist die Entropie groß.\n",
    "Ist das Ergebnis einer Zufallsvariable nahezu sicher (Ziehen einer Kugel aus einer Urne mit 9 schwarzen und einer roten Kugel), wird die Entropie klein.<br>\n",
    "Wir wollen die Unsicherheit unseres Klassifikationsbaums über die Labels in den Blättern bestimmen und minimieren.\n",
    "Die Entropie gibt uns eine einfache Möglichkeit dazu, da sie angewendet auf die Labels gerade diese Unsicherheit misst.\n",
    "Sind alle Labels unserer Trainingsdaten in einem Blatt gleich, können wir sehr sicher sagen, dass neue Daten in diesem Blatt das gleiche Label annehmen werden, und die Unsicherheit des Blatts ist sehr gering (Entropie gleich 0).\n",
    "Enthält ein Blatt einen Mix aus verschiedenen Labels, können wir nur eine sehr unsichere Aussage darüber treffen, welches Label neue Daten in diesem Blatt annehmen sollen und die Unsicherheit ist sehr groß (Entropie größer 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T14:59:51.515750817Z",
     "start_time": "2023-11-23T14:59:50.744300807Z"
    }
   },
   "outputs": [],
   "source": [
    "### Alle wichtigen imports; kann ergänzt werden\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Aufgabe 1\n",
    "def entropy(y):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Als nächstes benötigen wir die Funktion zur Berechnung des IG bei einem Split.\n",
    "Schreiben Sie also eine Funktion `info_gain`, die als Parameter einen Vektor mit Labels `y`, einen Vektor `x_j` mit dem Feature, das zum Split betrachtet werden soll, und den Wert `z` des Features, an dem der Split gesetzt werden soll, übergeben bekommt.\n",
    "\n",
    "Die Funktion soll die Formel für den IG aus dem oben angegeben Überblick des CART Algorithmus berechnen und zurückgeben. Zur Erinnerung, die Formel für den Split am Grenzwert $z$ des Features $x_{:,j}$ lautet: <br>\n",
    "\n",
    "$$H(Y)-p(x_{:,j} \\lt z)H(Y|x_{:,j} \\lt z)- p(x_{:,j} \\ge z)H(Y|x_{:,j} \\ge z)$$\n",
    "\n",
    "Die bedingte Entropie war definiert als: <br>\n",
    "$$H(Y|x_{:,j} \\lt z)=- \\sum_{k=1}^C{p(y=k|x_{:,j}<z)\\log_2{p(y=k|x_{:,j}<z)}}$$\n",
    "\n",
    "**Anschaulich:**<br>\n",
    "Der Information Gain beschreibt die Differenz der Unsicherheit, die wir vor und nach einem Split über die Labels haben.\n",
    "Da die Unsicherheit in den Labels durch den Split nicht schlechter, sondern (hoffentlich) besser wird, bezeichnet der IG die Menge an Informationen, die wir durch Kenntnis des Features $x_{:,j}$ (entweder kleiner oder größer $z$) über die Labels $Y$ erlangen.\n",
    "\n",
    "**Tipp:**<br>\n",
    "Sie können zur Berechnung der bedingten Entropie die Funktion `entropy` aus der letzten Aufgabe verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aufgabe 2\n",
    "def info_gain(y, x_j, z):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Für den CART Algorithmus benötigen wir genau den Split, der für den aktuellen Knoten den maximalen IG erreicht.\n",
    "Die nächste Funktion `max_info_gain`, die Sie implementieren sollen, findet genau diesen Split für einen gegebenen Vektor an Labels `y` und eine Matrix `x` mit den Features (Größe $n\\times m$ mit $n$ Datenpunkten und $m$ Features).\n",
    "\n",
    "Sie suchen in der Funktion also: $$\\max_{z\\in\\text{unique\\_values}(x_{:,j}), j=1,...,m} \\left[H(Y)-p(x_{:,j} \\lt z)H(Y|x_{:,j} \\lt z)- p(x_{:,j} \\ge z)H(Y|x_{:,j} \\ge z) \\right]$$\n",
    "\n",
    "Geben Sie sowohl den Index des Features und den optimalen Wert, für den der maximale IG erreicht wird, zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aufgabe 3\n",
    "def max_info_gain(y, x):\n",
    "    return 0, 0\n",
    "\n",
    "# Sie können folgenden Code verwenden, um Ihre Implementierung\n",
    "# mit den Ergebnissen aus Übung 8.1 zu vergleichen.\n",
    "y_8_1 = np.array([0, 0, 1, 1, 0])\n",
    "x_8_1 = np.array([[1, 0, 1, 1, 0], [1, 1, 0, 1, 0], [0, 1, 0, 1, 1]]).T\n",
    "print(f'Entropie der Größe \"Wandern gehen\": {entropy(y_8_1)}')\n",
    "i, z = max_info_gain(y_8_1, x_8_1)\n",
    "print(f'Der maximale IG für die Größe \"Wandern gehen\" wird mit Feature {i} an Split Value {z} erreicht')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Nun, da wir die Metrik implementiert und damit die Logik hinter der Aufteilung der Daten zur Verfügung haben, können wir uns dem eigentlichen Entscheidungsbaum widmen. Schauen Sie sich dazu den folgenden Code an, der einen Entscheidungsbaum in einer Python-Klasse codiert. Natürlich ist diese Implementierung noch nicht vollständig. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        # der Entscheidungsbaum als Dictionary\n",
    "        self.tree = {'depth': 0,  # Tiefe des Baumes, 0 = Wurzelknoten\n",
    "                     'feature': None,  # Teile an diesem Feature\n",
    "                     'split': None,  # Teile an diesem Wert\n",
    "                     'value': None,  # Mehrheitslabel in diesem Knoten\n",
    "                     'left': None,  # left/right = Kindknoten (neue Dictionaries)\n",
    "                     'right': None}\n",
    "\n",
    "    def fit(self, x, y, depth=0):\n",
    "        # Fall 1: dieser Knoten ist leer\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        # Fall 2: alle Labels im Knoten sind gleich\n",
    "        elif all(label == y[0] for label in y):\n",
    "            return {'depth': depth,\n",
    "                    'value': y[0],\n",
    "                    'left': None,\n",
    "                    'right': None}\n",
    "        # Fall 3: maximale Baumtiefe erreicht\n",
    "        elif depth > self.max_depth:\n",
    "            return None\n",
    "        # Fall 4: rekursiv den Baum erstellen\n",
    "        else:\n",
    "            ### Aufgabe 4a\n",
    "            raise NotImplementedError\n",
    "\n",
    "    ### Aufgabe 4b\n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klasse enthält ein Attribut `tree` als Python Dictionary, mit Eigenschaften wie Baumtiefe `depth` und Kindknoten `left` bzw. `right`.\n",
    "Die Methoden `fit` und `predict` sind nach den bekannten Methoden aus *sklearn* benannt. Diese Eigenschaft werden wir später ausnutzen.\n",
    "Die Implementierung dieser Methoden wird Ihre Aufgabe sein!\n",
    "\n",
    "Ihre Aufgabe besteht aus den folgenden zwei Teilen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4a) Vervollständigen Sie die Funktion **`fit`** in der obigen Klasse. Sie erstellt rekursiv einen Entscheidungsbaum. In jedem Rekursionsschritt wird der aktuelle \"Ast\" des Baumes in zwei neue Äste aufgeteilt, die jeweils mit dem rekursiven Aufruf der `fit` Methode weiter verfeinert werden.\n",
    "* Die Methode bekommt einen Datensatz `X`, den zugehörigen Labelvektor `Y` und die aktuelle Baumtiefe `depth` übergeben.\n",
    "* Die folgenden Sonderfälle, welche die Rekursion abbrechen, sind bereits implementiert:\n",
    " 1. Leerer Knoten\n",
    " 2. Maximale Baumtiefe erreicht\n",
    " 3. Alle Labels in den Daten sind gleich\n",
    "* Implementieren sie den \"Standardfall\": Nutzen Sie Ihre Methode `max_info_gain`, um herauszufinden an welchem Feature und welchem Wert Sie die Daten und Labels teilen müssen. Erstellen Sie anschließend einen neuen Knoten (ein Python-Dictionary) mit diesen Informationen.\n",
    "    * `depth` - aktuelle Tiefe des Baumes\n",
    "    * `feature` und `split` - die Ergebnisse der Methode `max_info_gain`\n",
    "    * `value` - das Mehrheitslabel in diesem Knoten\n",
    "    * Für die Attribute `left` und `right` im Dictionary wird die Methode **fit** schließlich rekursiv mit den geteilten Daten und `depth+1` aufgerufen.\n",
    "* Speichern Sie den Baum in der Klassenvariable `self.tree` ab, falls `depth==0`. Falls `depth` nicht null ist, geben Sie einfach den neu erstellten Knoten zurück."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4b) Implementieren Sie die Funktion **`predict`**.\n",
    "* Die Methode bekommt einen Datensatz `X` übergeben und soll diesen mit dem Entscheidungsbaum klassifizieren.\n",
    "* Durchlaufen Sie für jeden Datenpunkt von `X` den Baum bis Sie einen Blattknoten erreichen.\n",
    "    * Nutzen Sie dafür die Klassenvariable `self.tree`.\n",
    "    * Überprüfen Sie für den Datenpunkt, ob Sie links oder rechts 'abzweigen' müssen.\n",
    "* Speichern Sie den Wert (*value*), also das vorhergesagte Label dieses Knotens in einer Python-Liste ab und geben Sie diese schließlich zurück. Dies ist die Vorhersage/Klassifikation Ihres Entscheidungsbaumes für diesen Datensatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Training, Vorhersage und Visualisierung.\n",
    "\n",
    "Führen Sie die untenstehende Codezeile aus. Diese enthält eine Funktion zum Visualisieren der Regionen eines Entscheidungsbaumes für zwei Features.\n",
    "Weiterhin wird der Schwertliliendatensatz geladen und die beiden Features *Petal width* und *Sepal width* ausgewählt. Der Datensatz wird in Trainings- und Testset aufgeteilt.\n",
    "\n",
    "Ihre Aufgaben:\n",
    "\n",
    "1. Erstellen Sie mit Hilfe Ihrer Klasse `DecisionTreeClassifier` einen Entscheidungsbaum mit maximaler Tiefe (1,3,5,7,9).\n",
    "2. Trainieren Sie den Baum mit der `fit` Methode auf dem Trainingsset.\n",
    "3. Rufen Sie anschließend die Funktion `plot_decision_regions` mit dem Testset, Ihrem Klassenobjekt sowie der maximalen Baumtiefe auf.\n",
    "4. Beantworten Sie die folgenden Fragen:\n",
    "   * Was sehen Sie in den Plots für die unterschiedlichen Baumtiefen?\n",
    "   * Für welche Baumtiefe erhalten Sie die größte Genauigkeit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aufgabe 5\n",
    "# Lade die Daten\n",
    "iris = load_iris()\n",
    "pair = [1, 3]  # Petal width, sepal width als Beispiel zur Visualisierung.\n",
    "X = iris.data[:, pair]\n",
    "y = iris.target\n",
    "# 80% training und 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "\n",
    "# helper function to plot decision boundaries\n",
    "def plot_decision_regions(X, y, classifier, max_depth, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = np.array(classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T))\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    # figure settings\n",
    "    plt.figure(figsize=(5, 5), dpi=80)\n",
    "    # pair hardcoded\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "    plt.axis(\"tight\")\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.5, cmap=cmap, levels=2)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    y_pred = classifier.predict(X)\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    plt.title(\"Maximale Baumtiefe: %i, Genauigkeit: %.4f\" %\n",
    "              (max_depth, metrics.accuracy_score(y, y_pred)))\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=np.expand_dims(cmap(idx), 0),\n",
    "                    marker=markers[idx], label=iris.target_names[cl])\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) **[Optional]** Vergleichen Sie Ihre Implementierung und Ihr Ergebnis mit der offiziellen Implementierung. Nutzen Sie dazu die folgenden Methoden von *scikit-learn*:\n",
    " * Trainieren Sie einen Entscheidungsbaum mit `tree.DecisionTreeClassifier`([Link](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)) und der Methode `fit` analog zu Aufgabe (5).\n",
    "     * Geben Sie für den Parameter `criterion` 'entropy' an.\n",
    " * Treffen Sie eine Vorhersage auf dem Testset mit der Methode `predict`.\n",
    "     * Nutzen Sie `metrics.accuracy_score` ([Link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)), um die Genauigkeit zu bestimmen.\n",
    " * Bestimmen Sie die optimale Baumtiefe des Entscheidungsbaumes von *scikit-learn* mit `model_selection.GridSearchCV` ([Link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    " Beantworten Sie die folgenden Fragen:\n",
    " * Haben Sie die selbe Genauigkeit mit Ihrem Entscheidungsbaum erreicht?\n",
    " * Erzielt für diese Tiefe auch Ihr Entscheidungsbaum die besten Ergebnisse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aufgabe 6\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
